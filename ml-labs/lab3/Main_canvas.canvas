{
	"nodes":[
		{"id":"2a162c60007a8043","type":"text","text":"```python\n#Ячейка 1 — импорты и загрузка данных\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data # (150,4)\ny = iris.target # (150,)\n\nprint(\"X.shape =\", X.shape, \"y.shape =\", y.shape)\nprint(\"Классы (labels):\", np.unique(y))\nprint(\"Имена классов:\", iris.target_names)\n```","x":160,"y":340,"width":600,"height":580},
		{"id":"2f3c20af3b48ddeb","type":"text","text":"```python\n#Ячейка 2 — визуализация (sepal и petal) — опционально, но полезно\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='k')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\nplt.title('Sepal length vs Sepal width') \n\nplt.subplot(1,2,2)\nplt.scatter(X[:,2], X[:,3], c=y, cmap='viridis', edgecolor='k')\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.title('Petal length vs Petal width')\n\nplt.colorbar(ticks=[0,1,2], label='class')\nplt.tight_layout()\nplt.show()\n```","x":160,"y":1120,"width":1240,"height":1040},
		{"id":"a4996e154de6ae08","type":"text","text":"```python\n#Ячейка 3 — тестовый срез и проверка евклидовых расстояний между двумя векторами\ndataset = X[:150:15]\noutput = y[:150:15]\n\ndef euclidean_distance(row1, row2):\n\t#Евклидово расстояние между двумя векторами (1D numpy массивы).\n\treturn np.linalg.norm(row1 - row2)\n\nprint(\"Индексы, взятые в dataset:\", list(range(0,150,15)))\nprint(\"output (метки) для dataset:\", output)\nprint(\"Имена классов для этих примеров:\", [iris.target_names[l] for l in output])\n\nprint(\"\\nРасстояния dataset[i] -> dataset[5]:\")\nexpected = [3.59722114972099, 3.4899856733230297, 3.539774004085572, 3.66742416417845,\n2.128379665379276, 0.0, 1.1874342087037915, 2.5159491250818244, 1.6217274740226855, 2.2158519806160335]\n\nfor i in range(len(dataset)):\n\td = euclidean_distance(dataset[i], dataset[5])\n\tprint(d)\n\n# при желании можно сравнить с expected:\nprint(\"\\nРазницы с ожидаемым (computed - expected):\")\nfor i, (d,e) in enumerate(zip([euclidean_distance(dataset[i], dataset[5]) for i in range(len(dataset))], expected)):\n\tprint(i, d - e)\n```","x":160,"y":2280,"width":1080,"height":1280},
		{"id":"cd29302d962e74da","type":"text","text":"```python\n#Ячейка 4 — get_neighbors (возвращает (features, dist, label))\ndef get_neighbors(train_set, train_labels, test_row, num_neighbors):\n\tdistances = []\n\tfor i, train_row in enumerate(train_set):\n\t\tdist = euclidean_distance(train_row, test_row)\n\t\tdistances.append((train_row, dist, train_labels[i]))\n\tdistances.sort(key=lambda t: t[1])\n\treturn distances[:num_neighbors]\n\nneighbors = get_neighbors(dataset, output, dataset[5], 3)\nfor n in neighbors:\n\tprint(n)\n```","x":160,"y":3760,"width":820,"height":440},
		{"id":"609644208c368d14","type":"text","text":"```python\n#Ячейка 5 — predict_classification (голосование) и проверка\ndef predict_classification(train_set, train_labels, test_row, num_neighbors):\n\tneighbors = get_neighbors(train_set, train_labels, test_row, num_neighbors)\n\tlabels = [t[2] for t in neighbors]\n\tmost_common = Counter(labels).most_common()\n\t# самое частое, в случае ничьей — Counter.returned order gives tie by first seen;\n\t# чтобы стабильнее, выберем минимальный label среди самых частых:\n\tmax_count = most_common[0][1]\n\tcandidates = [label for label, count in most_common if count == max_count]\n\treturn min(candidates)\n\n# Проверка\nprediction = predict_classification(dataset, output, dataset[5], 3)\nprint('Expected %d, Got %d.' % (output[5], prediction))\n```","x":160,"y":4320,"width":807,"height":440},
		{"id":"4258529ac72edd8a","type":"text","text":"```python\n#Ячейка 6 — k_nearest_neighbors для набора тестовых объектов\ndef k_nearest_neighbors(train_set, train_labels, test_set, num_neighbors):\n\tpreds = []\n\tfor test_row in test_set:\n\t\tpreds.append(predict_classification(train_set, train_labels, test_row, num_neighbors))\n\treturn np.array(preds)\n\n# Быстрая проверка: на dataset против самого dataset с k=1 должно быть 100%\npreds_self = k_nearest_neighbors(dataset, output, dataset, 1)\nprint(\"Точность на наборе данных с k=1:\", (preds_self == output).mean())\n```","x":160,"y":4920,"width":870,"height":380},
		{"id":"dede6270b8aa7d19","type":"text","text":"```python\n#Ячейка 7 — Разбиение выборки, запуск нашего kNN и оценка\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)  \n\nnum_neighbors = 5\npredictions = k_nearest_neighbors(X_train, y_train, X_test, num_neighbors)\nacc = accuracy_score(y_test, predictions)\n\nprint(f\"Точность пользовательского kNN (k={num_neighbors}) для этого разделения: {acc:.4f} ({acc*100:.2f}%)\")\n```","x":160,"y":5440,"width":1080,"height":300},
		{"id":"a38b085f0943206f","type":"text","text":"```python\n#Ячейка 8 — Подбор k = 1..60 (одно разбиение) — график\ndef evaluate_k_range(X_train, y_train, X_test, y_test, k_max=60):\n\tks = list(range(1, k_max+1))\n\taccs = []\n\tfor k in ks:\n\t\tpreds = k_nearest_neighbors(X_train, y_train, X_test, k)\n\t\taccs.append(accuracy_score(y_test, preds))\n\treturn ks, accs\n\nks, accs = evaluate_k_range(X_train, y_train, X_test, y_test, k_max=60)\nplt.figure(figsize=(10,5))\nplt.plot(ks, accs, marker='o')\nplt.xlabel('k (число соседей)')\nplt.ylabel('Точность Accuracy на тесте')\nplt.title('Accuracy vs k (наш kNN) — split random_state=123')\nplt.grid(True)\nplt.show()\n\nbest_k = ks[np.argmax(accs)]\nprint(\"Лучший k на этом разбиении:\", best_k, \"accuracy =\", max(accs))\n```","x":160,"y":5920,"width":1040,"height":1120},
		{"id":"00f33774ea6beaf2","type":"text","text":"```python\n#Ячейка 9 — Проверка стабильности: несколько разбиений (средняя точность по разным seeds)\n\n# Список случайных seed-ов.\n# Каждый seed создаёт своё разбиение train/test,\n# что помогает понять, насколько результат устойчив к случайности.\n\nseeds = [1, 7, 42, 123, 2023]\nk_max = 30 # максимальное значение k для перебора\nmean_accs = np.zeros(k_max) # массив для накопления сумм accuracy по каждому k\n\n# Перебираем разные разбиения выборки\nfor seed in seeds:\n\t# Разбиваем данные при разных random_state\n\t# stratify=y гарантирует одинаковые пропорции классов в train и test\n\tX_train, X_test, y_train, y_test = train_test_split(\n\tX, y, test_size=0.2, random_state=seed, stratify=y\n\t)\n\t\n\t# Функция evaluate_k_range возвращает accuracy для каждого k (1..k_max)\n\tks, accs_seed = evaluate_k_range(X_train, y_train, X_test, y_test, k_max=k_max)\n\t\n\t# Накапливаем сумму accuracy для дальнейшего усреднения\n\tmean_accs += np.array(accs_seed)\n\n# Усредняем accuracy по количеству разбиений\nmean_accs /= len(seeds)\n\n# Строим график средней точности при разных k\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, k_max + 1), mean_accs, marker='o')\nplt.xlabel('k — число соседей')\nplt.ylabel('Средняя точность (accuracy)')\nplt.title('Усреднённая accuracy на разных разбиениях train/test')\nplt.grid(True)\nplt.show()\n\n# Выбор лучшего k (индекс + 1, т.к. индексация с 0)\nbest_k_mean = 1 + np.argmax(mean_accs)\nprint(\"Лучшее k по средней точности:\", best_k_mean)\n```","x":160,"y":7280,"width":1040,"height":1600},
		{"id":"df3ef69c3af49427","type":"text","text":"dataset = X[:150:15], output = y[:150:15], и вычисление расстояний к dataset[5]","x":1600,"y":2872,"width":360,"height":96},
		{"id":"5a85195e22fb7ee3","type":"text","text":"Возвращает список длины num_neighbors, где каждый элемент: (train_row (np.array), distance (float), label)","x":1320,"y":3903,"width":404,"height":115},
		{"id":"74a3b891d2fc0645","type":"text","text":"```python\n#Ячейка 10 — Сравнение с sklearn KNeighborsClassifier и LogisticRegression\n\n# Используем лучшее k, найденное в предыдущей ячейке\nsk_k = best_k_mean\n\n# 1. KNeighborsClassifier из sklearn\nknn = KNeighborsClassifier(n_neighbors=sk_k)\nknn.fit(X_train, y_train) # \"обучение\" (запоминание обучающей выборки)\n\n# Предсказания\nsk_preds = knn.predict(X_test)\n\n# Точность классификатора sklearn\nsk_acc = accuracy_score(y_test, sk_preds)\nprint(f\"Точность sklearn KNN при k={sk_k}: {sk_acc:.4f}\")\n  \n\n# 2. Логистическая регрессия из sklearn\n# max_iter увеличено, чтобы гарантировать сходимость алгоритма оптимизации\nlogreg = LogisticRegression(max_iter=2000)\nlogreg.fit(X_train, y_train)\n\n# Предсказания логистической регрессии\nlr_preds = logreg.predict(X_test)\n\n# Точность логистической регрессии\nlr_acc = accuracy_score(y_test, lr_preds)\nprint(f\"Точность Logistic Regression: {lr_acc:.4f}\")\n\n# Вывод: сравнение эффективности моделей\nprint(\"\\nСравнение моделей:\")\nprint(f\"KNN (sklearn, k={sk_k}) accuracy: {sk_acc:.4f}\")\nprint(f\"Logistic Regression accuracy: {lr_acc:.4f}\")\n```","x":160,"y":9060,"width":789,"height":1020},
		{"id":"55961d4c9cf68df3","type":"text","text":"Используем best_k_mean как рекомендованный k","x":1380,"y":9508,"width":250,"height":60},
		{"id":"a49a8d1d2e9828df","type":"text","text":"text","x":1630,"y":8013,"width":250,"height":60},
		{"id":"c1de5b34c608d8be","type":"text","text":"```python\nimport sys\nprint(sys.executable)\n\n```","x":1042,"y":340,"width":716,"height":261}
	],
	"edges":[
		{"id":"43f7111fddb79ec6","fromNode":"2a162c60007a8043","fromSide":"bottom","toNode":"2f3c20af3b48ddeb","toSide":"top"},
		{"id":"e08a41c5bdb68341","fromNode":"2f3c20af3b48ddeb","fromSide":"bottom","toNode":"a4996e154de6ae08","toSide":"top"},
		{"id":"e69e6125f6a11ed6","fromNode":"a4996e154de6ae08","fromSide":"bottom","toNode":"cd29302d962e74da","toSide":"top"},
		{"id":"78475e77aa2a9a63","fromNode":"cd29302d962e74da","fromSide":"bottom","toNode":"609644208c368d14","toSide":"top"},
		{"id":"70174d92a333a18f","fromNode":"609644208c368d14","fromSide":"bottom","toNode":"4258529ac72edd8a","toSide":"top"},
		{"id":"797b8a7e187cecb5","fromNode":"4258529ac72edd8a","fromSide":"bottom","toNode":"dede6270b8aa7d19","toSide":"top"},
		{"id":"c0968590e5149dc5","fromNode":"dede6270b8aa7d19","fromSide":"bottom","toNode":"a38b085f0943206f","toSide":"top"},
		{"id":"8e2c13b04ba5004d","fromNode":"a38b085f0943206f","fromSide":"bottom","toNode":"00f33774ea6beaf2","toSide":"top"},
		{"id":"e68477bceb264a94","fromNode":"00f33774ea6beaf2","fromSide":"bottom","toNode":"74a3b891d2fc0645","toSide":"top"},
		{"id":"1f32c2b491a1bde8","fromNode":"a4996e154de6ae08","fromSide":"right","toNode":"df3ef69c3af49427","toSide":"left"},
		{"id":"b517a6c2f815fb2f","fromNode":"cd29302d962e74da","fromSide":"right","toNode":"5a85195e22fb7ee3","toSide":"left"},
		{"id":"e86e0601bdd21fe9","fromNode":"74a3b891d2fc0645","fromSide":"right","toNode":"55961d4c9cf68df3","toSide":"left"},
		{"id":"781bf74d8e8b3f9b","fromNode":"00f33774ea6beaf2","fromSide":"right","toNode":"a49a8d1d2e9828df","toSide":"left"}
	]
}