{
	"nodes":[
		{"id":"2084c3d835410b35","type":"group","x":-827,"y":5920,"width":800,"height":360,"label":"3"},
		{"id":"207b82f65c7da1d7","type":"group","x":-827,"y":9280,"width":709,"height":327,"label":"5"},
		{"id":"6c7e90bd96a69a22","type":"group","x":-827,"y":8160,"width":610,"height":380,"label":"4"},
		{"id":"df2683d1ff6518fe","type":"group","x":-700,"y":2340,"width":582,"height":260,"label":"2"},
		{"id":"c520257f0befa2c3","type":"text","text":"5. Разобьем выборку на обучающую и тестовую. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","x":-807,"y":8180,"width":565,"height":101},
		{"id":"e907a771c8287f98","type":"text","text":"6. Зададим теперь произвольно num_neighbors и найдем предсказания predictions меток классов для тестовой выборки.","x":-807,"y":8330,"width":570,"height":77},
		{"id":"dede6270b8aa7d19","type":"text","text":"```python\n#Ячейка 7 — Разбиение выборки, запуск нашего kNN и оценка\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)  \n\nnum_neighbors = 5\npredictions = k_nearest_neighbors(X_train, y_train, X_test, num_neighbors)\nacc = accuracy_score(y_test, predictions)\n\nprint(f\"Точность пользовательского kNN (k={num_neighbors}) для этого разделения: {acc:.4f} ({acc*100:.2f}%)\")\n```","x":153,"y":8180,"width":1080,"height":300},
		{"id":"b713edd5e2bc2367","type":"text","text":"7. Найдите долю правильных ответов.","x":-807,"y":8455,"width":360,"height":50},
		{"id":"94fc030315863c05","type":"text","text":"\n\n- `train_test_split(...)`  \n    — делит исходный набор `X, y` на обучающую и тестовую части: 80% для обучения, 20% для теста.\n    \n    - `test_size=0.2` — доля теста 20%.\n        \n    - `random_state=123` — фиксирует \"случайное\" разбиение, чтобы результат был воспроизводимым.\n        \n    - `stratify=y` — гарантирует, что в train и test сохранится пропорция классов (важно для сбалансированных оценок).\n        \n- `num_neighbors = 5`  \n    — устанавливает число соседей `k` в k-NN (здесь — 5).\n    \n- `predictions = k_nearest_neighbors(X_train, y_train, X_test, num_neighbors)`  \n    — применяет твою реализацию k-NN ко всем объектам тестовой выборки `X_test`, возвращая массив предсказанных меток.\n    \n- `acc = accuracy_score(y_test, predictions)`  \n    — считает долю правильно предсказанных меток (accuracy): число совпавших / число объектов.\n    \n- `print(...)` — печатает значение accuracy в удобном формате (четыре знака после запятой и в процентах).","x":1880,"y":8180,"width":782,"height":500},
		{"id":"64672006a13b5c39","type":"text","text":"```\nВывод\n\nТочность пользовательского KNN (k=5) для этого разделения: 0,9333 (93.33%)\n```\n\n- `acc = 0.9333` означает, что на данном конкретном разбиении и при `k=5` модель правильно классифицировала **93.33%** объектов тестовой выборки. Для Iris это хороший результат: модель разделяет большинство примеров корректно.\n    \n- Важно понимать: это точность именно на **одном** разбиении (random_state=123). На другом разбиении значение может немного отличаться. Поэтому одна точность не даёт полной картины устойчивости модели.\n    \n\n### Что от этого следует (ключевые мысли)\n\n- `k=5` — типичное разумное значение для Iris; слишком маленькое `k` увеличивает чувствительность к шуму, слишком большое — сглаживает границы (bias).\n    \n- `stratify=y` + `random_state` дают воспроизводимость и корректность оценки (пропорции классов сохранены в тесте).\n    \n- Accuracy — простая метрика; для детального анализа стоит смотреть **confusion matrix** и/или precision/recall по классам, особенно если нужен анализ ошибок между `versicolor` и `virginica`.","x":3320,"y":8180,"width":712,"height":580},
		{"id":"5c431a74133fc56a","type":"text","text":"Мы перебираем `k = 1..k_max`, для каждого `k` предсказывает метки на единственном отложенном `X_test` (текущем разбиении) с помощью твоей реализации `k_nearest_neighbors` и собирает accuracy — затем строит график `accuracy vs k`.\n","x":1880,"y":9240,"width":782,"height":120},
		{"id":"93372dc96b34fa7d","type":"text","text":"### Что видно на графике\n\n- Для большинства `k` точность держится в диапазоне ≈ 0.90–0.94.\n    \n- Для `k = 11` наблюдается пик — **accuracy = 0.966666... (≈96.67%)**, то есть лучшая точность на этом конкретном тесте.\n    \n- При больших `k` (особенно >50) точность падает — это логично: модель «усредняет» слишком много соседей и теряет способность корректно разделять классы (увеличивается bias).\n    \n\nВывод из графика: на данном **единственном** разбиении `k=11` даёт максимальную accuracy.","x":3320,"y":9240,"width":712,"height":320},
		{"id":"5652c8807d124fce","type":"text","text":"реализуем функцию **k_nearest_neighbors**(train_set, labels, test, num_neighbors), которая получает на вход уже не один тестовый элемент, как предыдущие функции, а набор таких элементов (test), для каждого из них определяет класс и возвращает список предсказанных ответов.","x":-807,"y":7180,"width":800,"height":120},
		{"id":"8bf3860c00ba4d7e","type":"text","text":"predict_classification(train_set, labels, test_row, num_neighbors), возвращающей метку класса, которому вероятно принадлежит объект test_row. \n\npredict_classification будет вызывать предыдущую функцию get_neighbors, определять, какое из значений меток классов (0, 1, 2) встречается среди ближайших соседей наибольшее число раз и возвращать это значение.","x":-807,"y":5940,"width":760,"height":194},
		{"id":"74382a2168823243","type":"text","text":"4. Реализуйте функцию predict_classification(train_set, labels, test_row, num_neighbors) и проверьте ее работу на тестовом наборе данных.","x":-807,"y":6174,"width":673,"height":86},
		{"id":"cd29302d962e74da","type":"text","text":"```python\n#Ячейка 4 — get_neighbors (возвращает (features, dist, label))\ndef get_neighbors(train_set, train_labels, test_row, num_neighbors):\n\tdistances = []\n\tfor i, train_row in enumerate(train_set):\n\t\tdist = euclidean_distance(train_row, test_row)\n\t\tdistances.append((train_row, dist, train_labels[i]))\n\tdistances.sort(key=lambda t: t[1])\n\treturn distances[:num_neighbors]\n\nneighbors = get_neighbors(dataset, output, dataset[5], 3)\nfor n in neighbors:\n\tprint(n)\n```","x":153,"y":4240,"width":820,"height":440},
		{"id":"2f3c20af3b48ddeb","type":"text","text":"```python\n#Ячейка 2 — визуализация (sepal и petal) — опционально, но полезно\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='k')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\nplt.title('Sepal length vs Sepal width') \n\nplt.subplot(1,2,2)\nplt.scatter(X[:,2], X[:,3], c=y, cmap='viridis', edgecolor='k')\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.title('Petal length vs Petal width')\n\nplt.colorbar(ticks=[0,1,2], label='class')\nplt.tight_layout()\nplt.show()\n```","x":160,"y":1120,"width":1240,"height":1040},
		{"id":"4258529ac72edd8a","type":"text","text":"```python\n#Ячейка 6 — k_nearest_neighbors для набора тестовых объектов\ndef k_nearest_neighbors(train_set, train_labels, test_set, num_neighbors):\n\tpreds = []\n\tfor test_row in test_set:\n\t\tpreds.append(predict_classification(train_set, train_labels, test_row, num_neighbors))\n\treturn np.array(preds)\n\n# Быстрая проверка: на dataset против самого dataset с k=1 должно быть 100%\npreds_self = k_nearest_neighbors(dataset, output, dataset, 1)\nprint(\"Точность на наборе данных с k=1:\", (preds_self == output).mean())\n```","x":153,"y":7180,"width":870,"height":380},
		{"id":"609644208c368d14","type":"text","text":"```python\n#Ячейка 5 — predict_classification (голосование) и проверка\ndef predict_classification(train_set, train_labels, test_row, num_neighbors):\n\tneighbors = get_neighbors(train_set, train_labels, test_row, num_neighbors)\n\t\n\tlabels = [t[2] for t in neighbors]\n\tmost_common = Counter(labels).most_common()\n\t# самое частое, в случае ничьей — Counter.returned order gives tie by first seen;\n\t# чтобы стабильнее, выберем минимальный label среди самых частых:\n\tmax_count = most_common[0][1]\n\tcandidates = [label for label, count in most_common if count == max_count]\n\treturn min(candidates)\n\n# Проверка\nprediction = predict_classification(dataset, output, dataset[5], 3)\nprint('Expected %d, Got %d.' % (output[5], prediction))\n```","x":133,"y":5920,"width":807,"height":460},
		{"id":"a4996e154de6ae08","type":"text","text":"```python\n#Ячейка 3 — тестовый срез и проверка евклидовых расстояний между двумя векторами\ndataset = X[:150:15]\noutput = y[:150:15]\n\ndef euclidean_distance(row1, row2):\n\t#Евклидово расстояние между двумя векторами (1D numpy массивы).\n\treturn np.linalg.norm(row1 - row2)\n\nprint(\"Индексы, взятые в dataset:\", list(range(0,150,15)))\nprint(\"output (метки) для dataset:\", output)\nprint(\"Имена классов для этих примеров:\", [iris.target_names[l] for l in output])\n\nprint(\"\\nРасстояния dataset[i] -> dataset[5]:\")\nexpected = [3.59722114972099, 3.4899856733230297, 3.539774004085572, 3.66742416417845,\n2.128379665379276, 0.0, 1.1874342087037915, 2.5159491250818244, 1.6217274740226855, 2.2158519806160335]\n\nfor i in range(len(dataset)):\n\td = euclidean_distance(dataset[i], dataset[5])\n\tprint(d)\n\n# при желании можно сравнить с expected:\nprint(\"\\nРазницы с ожидаемым (computed - expected):\")\nfor i, (d,e) in enumerate(zip([euclidean_distance(dataset[i], dataset[5]) for i in range(len(dataset))], expected)):\n\tprint(i, d - e)\n```","x":160,"y":2280,"width":1080,"height":1280},
		{"id":"65f0389b24261d82","type":"text","text":"### Функция - predict_classification\n\n1. `neighbors = get_neighbors(...)`  \n    — находит `k` ближайших соседей к `test_row`. Каждый элемент `neighbors` — кортеж `(features, distance, label)`.\n    \n2. `labels = [t[2] for t in neighbors]`  \n    — извлекает из каждого кортежа только метки соседей. Пример: `[1, 1, 2]`.\n    \n3. `most_common = Counter(labels).most_common()`  \n    — `Counter` подсчитывает частоты меток; `most_common()` возвращает список пар `(label, count)` отсортированных по убыванию частоты. Для `[1,1,2]` это `[(1,2),(2,1)]`.\n    \n4. `max_count = most_common[0][1]`  \n    — берём максимальную частоту (сколько раз повторяется наиболее распространённая метка). В примере `max_count = 2`.\n    \n5. `candidates = [label for label, count in most_common if count == max_count]`  \n    — собираем все метки, у которых частота равна `max_count`. Это нужно на случай **ничьей** (например, при k=4 и метках `[0,0,1,1]` кандидаты будут `[0,1]`).\n    \n6. `return min(candidates)`  \n    — возвращаем минимальную метку среди кандидатов. Это простая детерминированная стратегия разрешения ничьих: всегда выбирать наименьшую числовую метку (например, из `[0,1]` вернём `0`). Такая политика даёт стабильный результат при повторных запусках.","x":1880,"y":5920,"width":782,"height":540},
		{"id":"e9e6880c10ae149d","type":"text","text":"## Функция `k_nearest_neighbors`\n\n1. Создаёт пустой список `preds`.\n    \n2. Для каждого элемента `test_row` из `test_set` вызывает `predict_classification(...)`, который:\n    \n    - находит `num_neighbors` ближайших соседей в `train_set`,\n        \n    - делает голосование по их меткам и возвращает предсказанную метку.\n        \n3. Добавляет предсказание в `preds`.\n    \n4. В конце преобразует список предсказаний в `numpy`-массив и возвращает его.\n    \n\nТаким образом `k_nearest_neighbors` — это простая обёртка, применяющая одноэлементную функцию предсказания ко всему набору тестовых объектов.","x":1880,"y":7180,"width":782,"height":340},
		{"id":"5a85195e22fb7ee3","type":"text","text":"Возвращает список длины num_neighbors, где каждый элемент: (train_row (np.array), distance (float), label)","x":1155,"y":4345,"width":404,"height":115},
		{"id":"df3ef69c3af49427","type":"text","text":"dataset = X[:150:15], output = y[:150:15], и вычисление расстояний к dataset[5]","x":1364,"y":2560,"width":360,"height":96},
		{"id":"932abff167ae294e","type":"text","text":"\n- `plt.figure(figsize=(12,5))` — задаёт местo для двух графиков (широкая фигура).\n    \n- `plt.subplot(1,2,1)` — первая панель (1 строка, 2 столбца, индекс 1).\n    \n- `plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='k')`:\n    \n    - `X[:,0]` — sepal length (ось X),\n        \n    - `X[:,1]` — sepal width (ось Y),\n        \n    - `c=y` — цвета точек соответствуют классам (0/1/2),\n        \n    - `cmap='viridis'` — цветовая карта,\n        \n    - `edgecolor='k'` — чёрная обводка для маркеров (улучшает читабельность).\n        \n- Подписи осей и заголовок делают график информативным.\n    \n- Аналогично для второй панели: `X[:,2]` и `X[:,3]` — petal length/width.\n    \n- `plt.colorbar(ticks=[0,1,2], label='class')` — легенда цветов; можно сопроводить текстовыми именами позже.\n    \n- `plt.tight_layout()` — аккуратно размещает элементы, чтобы метки не накладывались.\n    \n- `plt.show()` — отображает итог.\n    \n\nИнтерпретация графиков — что можно написать в отчёте\n\n- На графике **petal length vs petal width** видно хорошее разделение классов: _setosa_ явно отделён; это объясняет, почему классификаторы легко распознают setosa.\n    \n- На графике **sepal length vs sepal width** классы сильнее перекрываются — сложнее отличать versicolor и virginica.\n    \n- Вывод: petal-параметры более информативны для этой задачи.","x":1880,"y":1120,"width":764,"height":620},
		{"id":"e5c1a8f6d58f9c09","type":"text","text":"Снова произведите разбиение train_test_split выборки на тестовую и обучающую. Элементы выборок будут уже другие. Оцените точность классификатора.","x":-807,"y":9300,"width":576,"height":119},
		{"id":"2698b5694f05d872","type":"text","text":"8. Для выбора оптимального значения k постройте график зависимости точности классификатора от числа k=1,.., 60. Посмотрите на этот график при различных разбиениях на тестовую и обучающую выборку и сделайте вывод о наилучших значениях k в Вашей задаче.","x":-807,"y":9460,"width":669,"height":127},
		{"id":"a20e6232ba26972b","type":"text","text":"9. Сравните точность работы этих двух классификаторов.","x":-747,"y":10720,"width":488,"height":56},
		{"id":"f1198efaed394f24","type":"text","text":"```\nВывод\n\nX.shape = (150, 4)  y.shape = (150,)\nКлассы (labels): [0 1 2]\nИмена классов: ['setosa' 'versicolor' 'virginica']\n\n```\n\n- `X.shape = (150, 4)` — в наборе 150 образцов, у каждого 4 признака (sepal length, sepal width, petal length, petal width).\n    \n- `y.shape = (150,)` — 150 меток классов (по одному на образец).\n    \n- `labels [0 1 2]` — в данных три класса, кодированные числами 0,1,2.\n    \n- `iris.target_names` показывает, какой числовой метке соответствует название вида:  \n    `0 → setosa`, `1 → versicolor`, `2 → virginica`.  \n    Это базовая проверка корректной загрузки данных.","x":3320,"y":340,"width":708,"height":441},
		{"id":"2a162c60007a8043","type":"text","text":"```python\n#Ячейка 1 — импорты и загрузка данных\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data # (150,4)\ny = iris.target # (150,)\n\nprint(\"X.shape =\", X.shape, \"y.shape =\", y.shape)\nprint(\"Классы (labels):\", np.unique(y))\nprint(\"Имена классов:\", iris.target_names)\n```","x":160,"y":340,"width":600,"height":580},
		{"id":"ea19cb9a3c3022ad","type":"text","text":"# Что делает и зачем\n\n- `import numpy as np` — подключает NumPy для работы с массивами и векторами (базовая библиотека для ML).\n    \n- `import matplotlib.pyplot as plt` — подключает matplotlib для построения графиков.\n    \n- `from collections import Counter` — удобный счётчик (понадобится для голосования соседей в k-NN).\n    \n- `from sklearn.datasets import load_iris` — загружает встроенный набор Iris.\n    \n- `from sklearn.model_selection import train_test_split` — утилита для разбиения данных (будет использована позже).\n    \n- `from sklearn.neighbors import KNeighborsClassifier` — класс k-NN из sklearn (для сравнения/бенчмарка).\n    \n- `from sklearn.linear_model import LogisticRegression` — логистическая регрессия (сравнение).\n    \n- `from sklearn.metrics import accuracy_score` — функция для вычисления accuracy.\n    \n- `iris = load_iris()` — загружает объект с полями:\n    \n    - `iris.data` — матрица признаков (150×4),\n        \n    - `iris.target` — метки классов (150,),\n        \n    - `iris.feature_names`, `iris.target_names` — имена признаков и классов.\n        \n- `X = iris.data` / `y = iris.target` — сохраняем признаки и метки в привычные переменные.\n    \n- `print(...)` — быстрый контроль: форумы и список классов помогают убедиться, что данные загрузились корректно.\n    ","x":1880,"y":340,"width":764,"height":640},
		{"id":"a38b085f0943206f","type":"text","text":"```python\n#Ячейка 8 — Подбор k = 1..60 (одно разбиение) — график\ndef evaluate_k_range(X_train, y_train, X_test, y_test, k_max=60):\n\tks = list(range(1, k_max+1))\n\taccs = []\n\tfor k in ks:\n\t\tpreds = k_nearest_neighbors(X_train, y_train, X_test, k)\n\t\taccs.append(accuracy_score(y_test, preds))\n\treturn ks, accs\n\nks, accs = evaluate_k_range(X_train, y_train, X_test, y_test, k_max=60)\nplt.figure(figsize=(10,5))\nplt.plot(ks, accs, marker='o')\nplt.xlabel('k (число соседей)')\nplt.ylabel('Точность Accuracy на тесте')\nplt.title('Accuracy vs k (наш kNN) — split random_state=123')\nplt.grid(True)\nplt.show()\n\nbest_k = ks[np.argmax(accs)]\nprint(\"Лучший k на этом разбиении:\", best_k, \"accuracy =\", max(accs))\n```","x":153,"y":9240,"width":1040,"height":1120},
		{"id":"783ba0dfb1f5abc4","type":"text","text":"# тестовый срез и проверка евклидовых расстояний\n\n```python\ndataset = X[:150:15]\noutput = y[:150:15]\n```\n\n- Здесь берётся срез от `X` и `y`: каждый 15-й объект от 0 до 149 включительно.\n    \n- Индексы, которые попадают в `dataset` — `[0, 15, 30, 45, 60, 75, 90, 105, 120, 135]` (всего 10 образцов).\n    \n- `dataset` — массив формы `(10, 4)`, `output` — вектор `(10,)` с соответствующими метками классов.\n\n```python\ndef euclidean_distance(row1, row2):\n\t#Евклидово расстояние между двумя векторами (1D numpy массивы).\n\treturn np.linalg.norm(row1 - row2)\n```\n\n- `euclidean_distance` вычисляет L2-норму разницы векторов:  \n    $$\n    dist=\\sqrt{\\sum_i (row1_i - row2_i)^2}\n    $$\n- `np.linalg.norm` по умолчанию использует L2-норму, поэтому это правильная и компактная реализация евклидова расстояния.\n```python\nprint(\"Индексы, взятые в dataset:\", list(range(0,150,15)))\nprint(\"output (метки) для dataset:\", output)\nprint(\"Имена классов для этих примеров:\", [iris.target_names[l] for l in output])\n```\n\n- Выводит явный список индексов (для проверки корректности среза), метки классов для выбранных образцов и человеко-читаемые имена классов (setosa/versicolor/virginica) по этим меткам.\n```python\nprint(\"\\nРасстояния dataset[i] -> dataset[5]:\")\nexpected = [3.59722114972099, 3.4899856733230297, 3.539774004085572, 3.66742416417845,\n2.128379665379276, 0.0, 1.1874342087037915, 2.5159491250818244, 1.6217274740226855, 2.2158519806160335]\n\nfor i in range(len(dataset)):\n\td = euclidean_distance(dataset[i], dataset[5])\n\tprint(d)\n```\n- Для каждой строки `dataset[i]` вычисляется расстояние до `dataset[5]` (6-й элемент в срезе).\n    \n- Переменная `expected` содержит значения, приведённые в методичке — при корректной реализации и тех же данных печать `d` должна совпадать с этими числами с очень маленькой ошибкой плавающей точки.\n```python\nprint(\"\\nРазницы с ожидаемым (computed - expected):\")\nfor i, (d,e) in enumerate(zip([euclidean_distance(dataset[i], dataset[5]) for i in range(len(dataset))], expected)):\n\tprint(i, d - e)\n```\n- Здесь выводятся расхождения между вычисленными и ожидаемыми значениями (`computed - expected`).\n    \n- Из-за особенностей представления чисел с плавающей точкой эти разницы обычно равны нулю или очень близки к нулю (на уровне `1e-15`…`1e-12`), что подтверждает корректность реализации.","x":1880,"y":2280,"width":782,"height":1500},
		{"id":"00f33774ea6beaf2","type":"text","text":"```python\n#Ячейка 9 — Проверка стабильности: несколько разбиений (средняя точность по разным seeds)\n\nseeds = [1, 7, 42, 123, 2023]\nk_max = 30\nmean_accs = np.zeros(k_max)\n\nfor seed in seeds:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=seed, stratify=y\n    )\n    ks, accs_seed = evaluate_k_range(X_train, y_train, X_test, y_test, k_max=k_max)\n    mean_accs += np.array(accs_seed)\n\nmean_accs /= len(seeds)\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, k_max + 1), mean_accs, marker='o')\nplt.xlabel('k — число соседей')\nplt.ylabel('Средняя точность (accuracy)')\nplt.title('Усреднённая accuracy на разных разбиениях train/test')\nplt.grid(True)\nplt.show()\n\nbest_k_mean = 1 + np.argmax(mean_accs)\nprint(\"Лучшее k по средней точности:\", best_k_mean)\n\n```","x":153,"y":10640,"width":1040,"height":1260},
		{"id":"74a3b891d2fc0645","type":"text","text":"```python\n#Ячейка 10 — Сравнение с sklearn KNeighborsClassifier и LogisticRegression\n\nsk_k = best_k_mean\n\nknn = KNeighborsClassifier(n_neighbors=sk_k)\nknn.fit(X_train, y_train)\nsk_preds = knn.predict(X_test)\nsk_acc = accuracy_score(y_test, sk_preds)\nprint(f\"Точность sklearn KNN при k={sk_k}: {sk_acc:.4f}\")\n\nlogreg = LogisticRegression(max_iter=2000)\nlogreg.fit(X_train, y_train)\nlr_preds = logreg.predict(X_test)\nlr_acc = accuracy_score(y_test, lr_preds)\nprint(f\"Точность Logistic Regression: {lr_acc:.4f}\")\n\nprint(\"\\nСравнение моделей:\")\nprint(f\"KNN (sklearn, k={sk_k}) accuracy: {sk_acc:.4f}\")\nprint(f\"Logistic Regression accuracy: {lr_acc:.4f}\")\n\n```","x":153,"y":12320,"width":789,"height":680},
		{"id":"22d73c6cbb02a27a","type":"text","text":"## Формат возвращаемых данных\n\nКаждый элемент в возвращаемом списке — кортеж:\n```\n( numpy.ndarray (features), numpy.float64 (distance), numpy.int64 (label) )\n```\n\nТо есть:\n\n- первый элемент — NumPy-массив признаков соседа (например `[6.6, 3.0, 4.4, 1.4]`),\n    \n- второй — расстояние типа `np.float64`,\n    \n- третий — метка класса типа `np.int64`.\n    \n\n## Пояснение конкретного вывода\n\nВы вывели 3 ближайших соседа к `dataset[5]` и получили:\n```\n(array([6.6, 3. , 4.4, 1.4]), np.float64(0.0), np.int64(1))\n(array([5.5, 2.6, 4.4, 1.2]), np.float64(1.1874342087037915), np.int64(1))\n(array([6.9, 3.2, 5.7, 2.3]), np.float64(1.6217274740226855), np.int64(2))\n```\n\nРазберём по элементам:\n\n1. **Первый сосед**\n    \n    - Признаки: `[6.6, 3.0, 4.4, 1.4]`\n        \n    - Расстояние: `0.0` — это объект сам по себе (то есть `test_row` совпадает с этим `train_row`).\n        \n    - Метка: `1` — класс `versicolor`.  \n        → этот элемент — сам тестовый объект (поэтому расстояние 0).\n        \n2. **Второй сосед**\n    \n    - Признаки: `[5.5, 2.6, 4.4, 1.2]`\n        \n    - Расстояние ≈ `1.1874` — ближайший _не-сам_ сосед.\n        \n    - Метка: `1` — тоже `versicolor`.\n        \n3. **Третий сосед**\n    \n    - Признаки: `[6.9, 3.2, 5.7, 2.3]`\n        \n    - Расстояние ≈ `1.6217` — следующий по удалённости.\n        \n    - Метка: `2` — `virginica`.\n        \n\n## Что показывает этот вывод\n\n- Соседи упорядочены по расстоянию (от близкого к дальнему).\n    \n- Поскольку ближайшие соседи (`0.0` и `~1.187`) принадлежат классу `1`, это даёт основание для предсказания `1` методом голосования (kNN с k=3 — 2 из 3 соседей класса 1 → предсказание 1).\n    \n- Типы (`np.float64`, `np.int64`) — просто отражение того, что внутри используются объекты NumPy; это нормально и ожидаемо.","x":3320,"y":4240,"width":712,"height":1200},
		{"id":"b0978cf30d1275f3","type":"text","text":"- `distances = []` — создаём пустой список, в который будем собирать кортежи `(train_row, distance, label)`.\n    \n- `for i, train_row in enumerate(train_set):` — перебираем все строки обучающей выборки (здесь `train_set = dataset`).\n    \n- `dist = euclidean_distance(train_row, test_row)` — считаем евклидово расстояние между текущим обучающим примером и тестовым вектором `test_row`.\n    \n- `distances.append((train_row, dist, train_labels[i]))` — добавляем кортеж: сам вектор признаков соседа, числовое расстояние и метку этого соседа.\n    \n- `distances.sort(key=lambda t: t[1])` — сортируем список кортежей по второму элементу (по расстоянию), от ближайшего к дальнему.\n    \n- `return distances[:num_neighbors]` — возвращаем первые `num_neighbors` элементов — то есть `k` ближайших соседей.","x":1880,"y":4240,"width":782,"height":360},
		{"id":"41f8ebad95b732a0","type":"text","text":"```\nВывод\n\nИндексы, взятые в dataset: [0, 15, 30, 45, 60, 75, 90, 105, 120, 135]\noutput (метки) для dataset: [0 0 0 0 1 1 1 2 2 2]\nИмена классов для этих примеров: [ 'setosa','setosa','setosa','setosa','versicolor','versicolor','versicolor','virginica','virginica','virginica' ]\n\n```\n\n- `dataset = X[:150:15]` взял каждый 15-й образец: индексы указаны явно — `[0,15,30,...,135]`. Это тестовый поднабор из 10 образцов, по одному (в данном случае — по несколько) из каждой группы.\n    \n- `output` показывает метки этих 10 образцов: первые 4 — `0` (setosa), затем 3 — `1` (versicolor), затем 3 — `2` (virginica). Это ожидаемая структура, потому что в исходном порядке Iris: первые 50 — setosa, 50–99 — versicolor, 100–149 — virginica.\n    \n\nДалее — расстояния до `dataset[5]` (то есть до 6-го образца в этом срезе; в оригинале это объект с индексом `75`):\n```\nРасстояния dataset[i] -> dataset[5]:\n3.59722114972099\n3.4899856733230297\n3.539774004085572\n3.66742416417845\n2.128379665379276\n0.0\n1.1874342087037915\n2.5159491250818244\n1.6217274740226855\n2.2158519806160335\n```\n\n- Значение `0.0` для `i=5` — это расстояние объекта до самого себя (ожидаемо).\n    \n- Чем меньше число — тем ближе в признаковом пространстве соответствующий образец к `dataset[5]`.\n    \n- В этой последовательности ближайший _не-сам_ сосед — `1.1874` (i=6), у которого метка `1` (то есть ближайший сосед принадлежит тому же классу `versicolor` — хорошая вещь для классификации). Дальше идут другие образцы, расположенные дальше в порядке возрастания расстояния.\n    \n\nНаглядная упорядоченная картина (по возрастанию расстояния, исключая себя) — показывает, какие примеры являются ближайшими и к каким классам они принадлежат; в нашем случае ближайшие соседи к `dataset[5]` в основном имеют ту же метку `1`, что подтверждает корректность данных и метрики.","x":3320,"y":2280,"width":712,"height":1120},
		{"id":"94523699cf3a5adf","type":"text","text":"График - два scatter-плота: sepal length vs sepal width и petal length vs petal width\n\n- По **petal length / petal width** классы хорошо разделяются: `setosa` обычно явно отделён от остальных — поэтому многие модели легко распознают `setosa`.\n    \n- По **sepal length / sepal width** классы сильнее перекрываются — `versicolor` и `virginica` трудно разделимы по этим признакам.  \n    Эти наблюдения объясняют поведение классификаторов на Iris: простые линейные модели хуже отличают 1 и 2 классы, а non-linear / локальные методы (kNN, деревья) обычно справляются лучше.","x":3320,"y":1120,"width":712,"height":266},
		{"id":"f5fae8d0cd79ba7c","type":"text","text":"Ранее (в блоке 4) `get_neighbors(dataset, output, dataset[5], 3)` вернул соседей с метками `[1,1,2]` (сам объект + ближайший сосед того же класса + следующий сосед другого класса).  \nВ `labels = [1,1,2]` метка `1` встречается 2 раза → `max_count = 2` → candidates = `[1]` → функция возвращает `1`.  \nПоэтому вывод:\n```\nExpected 1, Got 1.\n```\n\nозначает, что предсказание совпало с истинной меткой `output[5]` — функция работает правильно на этом примере.","x":3320,"y":5920,"width":712,"height":320},
		{"id":"9966f5eaddac98be","type":"text","text":"```\nВывод\n\nТочность на наборе данных с k=1: 1.0\n```\n\n- Выражение `(preds_self == output).mean()` сравнивает поэлементно массив предсказаний `preds_self` и истинных меток `output`, получает булев массив (True/False), а `.mean()` вычисляет долю `True` — то есть долю правильных ответов (accuracy).\n    \n- Значение `1.0` означает `100%` точности.\n## Почему так получилось (логическое объяснение)\n\n- Вы сделали **проверку на том же наборе**, что и обучающий: `train_set = dataset` и `test_set = dataset`.\n    \n- При `k=1` ближайший сосед любого тестового образца — **он сам** (расстояние до себя = 0), поэтому `predict_classification` вернёт его же метку. В результате для каждого примера предсказание совпадает с истинной меткой → 100% точности.\n    \n- Это _ожидаемое и корректное_ поведение для самопроверки, но **не является показателем обобщающей способности** модели.\n## Важное замечание (практическое)\n\n- **k=1 и тест = train → тривиально 100%**, поэтому такая проверка — только sanity-check, а не оценка качества. Для честной оценки используйте отдельный тест-набор или кросс-валидацию (StratifiedKFold / `train_test_split` с `stratify=y`).","x":3320,"y":7180,"width":712,"height":700},
		{"id":"6d57044eb68cf1d1","type":"text","text":"2. Создайте массивы X и Y входных данных и меток классов, соответственно.","x":-680,"y":2360,"width":385,"height":93},
		{"id":"81996e3de64e1ccf","type":"text","text":"Далее нам потребуется функция **get_neighbors**(train_set, labels, test_row, num_neighbors), которая находит в train_set выборке k = num_neighbors ближайших соседей (в смысле близости евклидова расстояния) к даному (test_row).","x":-687,"y":4263,"width":720,"height":120},
		{"id":"8d78d76fa7ab5ab1","type":"text","text":"3. Реализуйте функцию, вычисляющую евклидово расстояние между двумя векторами euclidean_distance(row1, row2).","x":-680,"y":2480,"width":542,"height":78},
		{"id":"55961d4c9cf68df3","type":"text","text":"Используем best_k_mean как рекомендованный k","x":1280,"y":12560,"width":250,"height":60},
		{"id":"acafe53d360e9525","type":"file","file":"mashinoe/ml-labs/lab3/пр3.pdf","x":-2880,"y":370,"width":820,"height":600},
		{"id":"a49a8d1d2e9828df","type":"text","text":"1. **Перебор seed-ов**  \n    Ты создаёшь несколько различных разбиений выборки `train/test` (5 разных random_state).  \n    Это позволяет проверить устойчивость модели к случайности и уменьшить влияние удачного/неудачного разбиения.\n    \n2. **Для каждого разбиения**\n    \n    - С помощью `train_test_split` создаются новые выборки.\n        \n    - Для каждого значения `k` от `1` до `30` запускается `evaluate_k_range`.\n        \n    - Accuracy для каждого `k` накапливается в массив `mean_accs`.\n        \n3. **Усреднение результата**  \n    После всех seed-ов ты делишь сумму accuracy на количество разбиений → получаешь устойчивую среднюю оценку качества модели для каждого `k`.\n    \n4. **Построение графика**  \n    График показывает, как средняя точность меняется в зависимости от числа соседей.\n    \n5. **Выбор лучшего k**  \n    `argmax` возвращает индекс максимальной точности, поэтому добавляется `+1` (так как k начинается с 1, а индекс — с 0).","x":1880,"y":10640,"width":782,"height":460},
		{"id":"70818a14f226461d","type":"text","text":"\n### **1. Оптимальное значение k ≈ 4**\n\nАлгоритм выбрал:\n```\nЛучшее k по средней точности: 4\n```\n\nЭто видно и по кривой — в районе k=4 наблюдается наибольшая средняя accuracy, около **0.98**.\n\n### **2. Поведение accuracy по мере увеличения k**\n\n- При **малых k (1–5)** точность самая высокая.\n    \n- В диапазоне **6–12** точность слегка колеблется, но уже немного хуже.\n    \n- Начиная примерно с **k > 15**, точность заметно снижается.\n    \n- При **k > 20** точность падает до ~0.93–0.94 и стабильно держится ниже.\n    \n\nЭто абсолютно нормальное поведение для kNN:\n\n- **Малые k — модель чувствительна, но точность может быть высокой.**\n    \n- **Слишком большие k — алгоритм становится слишком \"усреднённым\" и теряет точность.**\n    \n\n### **3. График показывает хорошую устойчивость**\n\nКолебания небольшие → значит, результаты мало зависят от случайного разбиения.","x":3320,"y":10640,"width":712,"height":680},
		{"id":"ceb46af512f0643c","type":"text","text":"```\nВывод\n\nТочность sklearn KNN при k=4: 1.0000\n\nТочность Logistic Regression: 0.9667\n\nСравнение моделей:\n\nKNN (sklearn, k=4) accuracy: 1.0000\n\nLogistic Regression accuracy: 0.9667\n```\n\nНа тестовой выборке были получены следующие результаты:\n\n- **Точность KNN (k = 4): 1.0000**\n    \n- **Точность Logistic Regression: 0.9667**\n    \n\n### Интерпретация результатов\n\n1. **Модель KNN показала идеальную точность (100%)**  \n    Это означает, что при k = 4 алгоритм корректно классифицировал _все_ объекты тестовой выборки.  \n    Такой результат согласуется с предыдущим анализом: при усреднении по разным разбиениям k = 4 действительно давал одну из лучших точностей.\n    \n2. **Логистическая регрессия показала точность 0.9667**  \n    Она ошиблась примерно в 3,3% случаев.  \n    Для линейной модели это хороший результат, но ниже, чем у KNN. Это говорит о том, что:\n    \n    - классы в данных разделяются **нелинейно**,\n        \n    - модель KNN, которая использует локальную структуру данных, лучше адаптируется к форме распределения.\n        \n3. **Итог сравнения**\n    \n    - **KNN превосходит логистическую регрессию** на данном наборе данных.\n        \n    - Разница в точности — **3.33 процентных пункта**, что является существенным для задач бинарной классификации.\n        \n    - Идеальный результат KNN подтверждает корректность реализации и адекватность выбора k.","x":3320,"y":12320,"width":712,"height":980},
		{"id":"8df3cefc2ae2c791","type":"text","text":"1. **Используется найденный ранее оптимальный `k`**  \n    Значение `best_k_mean`, вычисленное по усреднённой accuracy, применяется как параметр для `KNeighborsClassifier`.\n    \n2. **Обучение KNN (sklearn)**  \n    `fit(X_train, y_train)` для kNN просто сохраняет обучающую выборку.  \n    Потом делается `predict` и вычисляется точность.\n    \n3. **Обучение Logistic Regression**  \n    Логистическая регрессия — линейная модель.  \n    Увеличение `max_iter=2000` обеспечивает сходимость.\n    \n4. **Обе модели тестируются на одном и том же тестовом наборе**, что делает сравнение корректным.\n    \n5. **Выводит accuracy двух моделей рядом**, что помогает оценить:\n    \n    - насколько хорошо работает kNN при оптимальном k,\n        \n    - превосходит ли логистическая регрессия (или наоборот).","x":1880,"y":12320,"width":782,"height":420},
		{"id":"c1de5b34c608d8be","type":"text","text":"```python\nimport sys\nprint(sys.executable)\n\n```","x":-1380,"y":-240,"width":716,"height":261},
		{"id":"a4fc42caac204bd4","type":"text","text":"1. Загрузите и изучите данные.","x":-680,"y":610,"width":300,"height":60},
		{"id":"52d89670d740324e","type":"file","file":"mashinoe/ml-labs/lab3/ТЗ - Лабораторная работа №3.md","x":-2880,"y":-320,"width":820,"height":549}
	],
	"edges":[
		{"id":"43f7111fddb79ec6","fromNode":"2a162c60007a8043","fromSide":"bottom","toNode":"2f3c20af3b48ddeb","toSide":"top"},
		{"id":"e08a41c5bdb68341","fromNode":"2f3c20af3b48ddeb","fromSide":"bottom","toNode":"a4996e154de6ae08","toSide":"top"},
		{"id":"e69e6125f6a11ed6","fromNode":"a4996e154de6ae08","fromSide":"bottom","toNode":"cd29302d962e74da","toSide":"top"},
		{"id":"78475e77aa2a9a63","fromNode":"cd29302d962e74da","fromSide":"bottom","toNode":"609644208c368d14","toSide":"top"},
		{"id":"70174d92a333a18f","fromNode":"609644208c368d14","fromSide":"bottom","toNode":"4258529ac72edd8a","toSide":"top"},
		{"id":"797b8a7e187cecb5","fromNode":"4258529ac72edd8a","fromSide":"bottom","toNode":"dede6270b8aa7d19","toSide":"top"},
		{"id":"c0968590e5149dc5","fromNode":"dede6270b8aa7d19","fromSide":"bottom","toNode":"a38b085f0943206f","toSide":"top"},
		{"id":"8e2c13b04ba5004d","fromNode":"a38b085f0943206f","fromSide":"bottom","toNode":"00f33774ea6beaf2","toSide":"top"},
		{"id":"e68477bceb264a94","fromNode":"00f33774ea6beaf2","fromSide":"bottom","toNode":"74a3b891d2fc0645","toSide":"top"},
		{"id":"1f32c2b491a1bde8","fromNode":"a4996e154de6ae08","fromSide":"right","toNode":"df3ef69c3af49427","toSide":"left"},
		{"id":"b517a6c2f815fb2f","fromNode":"cd29302d962e74da","fromSide":"right","toNode":"5a85195e22fb7ee3","toSide":"left"},
		{"id":"e86e0601bdd21fe9","fromNode":"74a3b891d2fc0645","fromSide":"right","toNode":"55961d4c9cf68df3","toSide":"left"},
		{"id":"781bf74d8e8b3f9b","fromNode":"00f33774ea6beaf2","fromSide":"right","toNode":"a49a8d1d2e9828df","toSide":"left"},
		{"id":"9e58891cb9e83b4d","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"a4fc42caac204bd4","toSide":"left"},
		{"id":"8a6ec523f451f78f","fromNode":"a4fc42caac204bd4","fromSide":"right","toNode":"2a162c60007a8043","toSide":"left"},
		{"id":"07c2223ebdfff33a","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"df2683d1ff6518fe","toSide":"left"},
		{"id":"1637996eb68f7a80","fromNode":"df2683d1ff6518fe","fromSide":"right","toNode":"a4996e154de6ae08","toSide":"left"},
		{"id":"c3aa3b6f0ecbe48d","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"81996e3de64e1ccf","toSide":"left"},
		{"id":"780ecb208ba73fd2","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"2084c3d835410b35","toSide":"left"},
		{"id":"ee96ef62007c0b7e","fromNode":"81996e3de64e1ccf","fromSide":"right","toNode":"cd29302d962e74da","toSide":"left"},
		{"id":"e0dbf0ecbcadde72","fromNode":"2084c3d835410b35","fromSide":"right","toNode":"609644208c368d14","toSide":"left"},
		{"id":"2fcc7bce9e9764e5","fromNode":"5652c8807d124fce","fromSide":"right","toNode":"4258529ac72edd8a","toSide":"left"},
		{"id":"0ea9f5094e5431c6","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"5652c8807d124fce","toSide":"left"},
		{"id":"b57d8a1bcc1ea564","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"6c7e90bd96a69a22","toSide":"left"},
		{"id":"391752d3ab1b27d6","fromNode":"6c7e90bd96a69a22","fromSide":"right","toNode":"dede6270b8aa7d19","toSide":"left"},
		{"id":"d65256916134b065","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"207b82f65c7da1d7","toSide":"left"},
		{"id":"38fac403b3acae9a","fromNode":"acafe53d360e9525","fromSide":"right","toNode":"a20e6232ba26972b","toSide":"left"},
		{"id":"72a439196f36a383","fromNode":"207b82f65c7da1d7","fromSide":"right","toNode":"a38b085f0943206f","toSide":"left"},
		{"id":"b7a6b028b92113aa","fromNode":"a20e6232ba26972b","fromSide":"right","toNode":"00f33774ea6beaf2","toSide":"left"},
		{"id":"dc56c64806f9b744","fromNode":"a20e6232ba26972b","fromSide":"right","toNode":"74a3b891d2fc0645","toSide":"left"},
		{"id":"6dca32a7a9f21d1f","fromNode":"a4fc42caac204bd4","fromSide":"right","toNode":"2f3c20af3b48ddeb","toSide":"left"},
		{"id":"0a1e43abe75a82f2","fromNode":"2a162c60007a8043","fromSide":"right","toNode":"ea19cb9a3c3022ad","toSide":"left"},
		{"id":"6af0dfd09a1e35b0","fromNode":"2f3c20af3b48ddeb","fromSide":"right","toNode":"932abff167ae294e","toSide":"left"},
		{"id":"ebe73fd24a48b9c6","fromNode":"df3ef69c3af49427","fromSide":"right","toNode":"783ba0dfb1f5abc4","toSide":"left"},
		{"id":"51e8cebed15fc699","fromNode":"ea19cb9a3c3022ad","fromSide":"right","toNode":"f1198efaed394f24","toSide":"left"},
		{"id":"5de7ea981cec9d2b","fromNode":"932abff167ae294e","fromSide":"right","toNode":"94523699cf3a5adf","toSide":"left"},
		{"id":"bc71f83bcd45ea84","fromNode":"783ba0dfb1f5abc4","fromSide":"right","toNode":"41f8ebad95b732a0","toSide":"left"},
		{"id":"532487d28b7d4bd3","fromNode":"5a85195e22fb7ee3","fromSide":"right","toNode":"b0978cf30d1275f3","toSide":"left"},
		{"id":"d83cc03a7f428224","fromNode":"b0978cf30d1275f3","fromSide":"right","toNode":"22d73c6cbb02a27a","toSide":"left"},
		{"id":"6b1a43d83e64c5c8","fromNode":"609644208c368d14","fromSide":"right","toNode":"65f0389b24261d82","toSide":"left"},
		{"id":"8b4bb395660326f5","fromNode":"65f0389b24261d82","fromSide":"right","toNode":"f5fae8d0cd79ba7c","toSide":"left"},
		{"id":"68e865ec567ccb6f","fromNode":"4258529ac72edd8a","fromSide":"right","toNode":"e9e6880c10ae149d","toSide":"left"},
		{"id":"0d770e80fa56e8e1","fromNode":"e9e6880c10ae149d","fromSide":"right","toNode":"9966f5eaddac98be","toSide":"left"},
		{"id":"b2ac856fab498c27","fromNode":"dede6270b8aa7d19","fromSide":"right","toNode":"94fc030315863c05","toSide":"left"},
		{"id":"cd80648c3287d45c","fromNode":"94fc030315863c05","fromSide":"right","toNode":"64672006a13b5c39","toSide":"left"},
		{"id":"e228b0d2dfb35439","fromNode":"a38b085f0943206f","fromSide":"right","toNode":"5c431a74133fc56a","toSide":"left"},
		{"id":"21c0f25b42d854d7","fromNode":"5c431a74133fc56a","fromSide":"right","toNode":"93372dc96b34fa7d","toSide":"left"},
		{"id":"3d23db0ee66449be","fromNode":"a49a8d1d2e9828df","fromSide":"right","toNode":"70818a14f226461d","toSide":"left"},
		{"id":"c987dc6a5d9c435a","fromNode":"55961d4c9cf68df3","fromSide":"right","toNode":"8df3cefc2ae2c791","toSide":"left"},
		{"id":"85ce79a7c94fa9ed","fromNode":"8df3cefc2ae2c791","fromSide":"right","toNode":"ceb46af512f0643c","toSide":"left"}
	]
}