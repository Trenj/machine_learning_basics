{
	"nodes":[
		{"id":"c0fcdf6f3b37c006","type":"text","text":"```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n\niris = load_iris()\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\n# стратифицированное разбиение\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n```","x":1280,"y":-645,"width":683,"height":580},
		{"id":"7e3a95c99197c4bc","type":"text","text":"```python\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# дерево без ограничений (может переобучить)\ndt_full = DecisionTreeClassifier(random_state=42)\ndt_full.fit(X_train, y_train)\n\ntrain_acc = accuracy_score(y_train, dt_full.predict(X_train))\ntest_acc = accuracy_score(y_test, dt_full.predict(X_test))\nprint(\"Неограниченное дерево - train acc: {:.4f}, test acc: {:.4f}\".format(train_acc, test_acc))\n\n# Визуализация ограниченного примера дерева (ограничим глубину для наглядности)\nplt.figure(figsize=(12,6))\nplot_tree(dt_full, feature_names=feature_names, class_names=target_names, filled=True, max_depth=3)\nplt.title(\"Неограниченное дерево решений\")\nplt.show()\n```","x":1280,"y":87,"width":1240,"height":1115},
		{"id":"98c19f21f60a47a4","type":"text","text":"```python\nfor params in [\n\t{\"max_depth\": 2},\n\t{\"max_depth\": 3},\n\t{\"max_depth\": 4},\n\t{\"min_samples_leaf\": 5},\n\t{\"min_samples_split\": 5}\n]:\n\n\tdt = DecisionTreeClassifier(random_state=42, **params)\n\tdt.fit(X_train, y_train)\n\n\tprint(f\"Params: {params} -> train_acc={accuracy_score(y_train, dt.predict(X_train)):.4f}, test_acc={accuracy_score(y_test, dt.predict(X_test)):.4f}\")\n```","x":1280,"y":1360,"width":840,"height":520},
		{"id":"08420326263ddaa2","type":"text","text":"```python\n# Ячейка: сравнение переобученного дерева и дерева с ограниченной глубиной (визуализация и accuracy)\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# Предполагается, что X_train, y_train, X_test, y_test, feature_names, target_names уже определены\n\n# 1) уже обученное ранее дерево без ограничений (если нет — обучим его)\ndt_unrestricted = DecisionTreeClassifier(random_state=42)\ndt_unrestricted.fit(X_train, y_train)\n\n# 2) дерево с ограниченной глубиной (pruned / regularized)\nmax_depth_limited = 3 # можно изменить (2,3,4...)\ndt_limited = DecisionTreeClassifier(max_depth=max_depth_limited, random_state=42)\ndt_limited.fit(X_train, y_train)\n\n# Печатаем accuracy для сравнения\ntrain_acc_un = accuracy_score(y_train, dt_unrestricted.predict(X_train))\ntest_acc_un = accuracy_score(y_test, dt_unrestricted.predict(X_test))\ntrain_acc_lim = accuracy_score(y_train, dt_limited.predict(X_train))\ntest_acc_lim = accuracy_score(y_test, dt_limited.predict(X_test))\n\nprint(f\"Unrestricted tree -> train acc: {train_acc_un:.4f}, test acc: {test_acc_un:.4f}\")\nprint(f\"Limited tree (max_depth={max_depth_limited}) -> train acc: {train_acc_lim:.4f}, test acc: {test_acc_lim:.4f}\")\n\n# Рисуем два дерева рядом\nplt.figure(figsize=(18, 8))\n\nplt.subplot(1, 2, 1)\n\n# Показываем первые уровни переобученного дерева, чтобы график не был нечитаем\nplot_tree(dt_unrestricted, feature_names=feature_names, class_names=target_names, filled=True, max_depth=3)\nplt.title(\"Переобученное дерево\")\n\nplt.subplot(1, 2, 2)\n\n# Полная отрисовка ограниченного дерева (т.к. глубина маленькая, отрисовка читаема)\nplot_tree(dt_limited, feature_names=feature_names, class_names=target_names, filled=True)\nplt.title(f\"Обрезанное дерево (max_depth={max_depth_limited})\")\n\nplt.tight_layout()\nplt.show()\n```","x":1280,"y":2080,"width":1823,"height":1990},
		{"id":"a4289d9c8d58cc13","type":"text","text":"```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\nprint(\"RandomForest (n_estimators=100) -> train acc: {:.4f}, test acc: {:.4f}\".format(\n\taccuracy_score(y_train, rf.predict(X_train)), accuracy_score(y_test, rf.predict(X_test))\n))\n\n# feature importances\nimportances = rf.feature_importances_\nfor name, imp in zip(feature_names, importances):\n\tprint(f\"{name}: {imp:.3f}\")\nplt.figure(figsize=(6,3))\nplt.barh(feature_names, importances)\nplt.title(\"Feature importances (RandomForest)\")\nplt.show()\n```","x":1280,"y":4300,"width":857,"height":933},
		{"id":"ffe9cba4615b37ee","type":"text","text":"```python\nn_estimators_list = [1, 5, 10, 50, 100, 200]\nmax_depth_list = [None, 2, 3, 4, 5]\n\nresults = []\nfor n in n_estimators_list:\n\tfor d in max_depth_list:\n\t\trf = RandomForestClassifier(n_estimators=n, max_depth=d, random_state=42, n_jobs=-1)\n\t\trf.fit(X_train, y_train)\n\t\tresults.append((n, d, accuracy_score(y_train, rf.predict(X_train)), accuracy_score(y_test, rf.predict(X_test))))\n\n# Отобразим матрицу результатов (test accuracy)\nimport pandas as pd\ndf = pd.DataFrame(results, columns=[\"n_estimators\",\"max_depth\",\"train_acc\",\"test_acc\"])\npivot = df.pivot(index=\"n_estimators\", columns=\"max_depth\", values=\"test_acc\")\nprint(\"Test accuracy (rows: n_estimators, cols: max_depth):\")\n\n#display(pivot)\nprint(pivot)\n```","x":1264,"y":5440,"width":1096,"height":720},
		{"id":"23544791ea73694a","type":"text","text":"```python\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb.fit(X_train, y_train)\nprint(\"GradientBoosting (100, lr=0.1, depth=3) -> train acc: {:.4f}, test acc: {:.4f}\".format(\naccuracy_score(y_train, gb.predict(X_train)), accuracy_score(y_test, gb.predict(X_test))\n))\n\n# feature importances\nimportances_gb = gb.feature_importances_\nfor name, imp in zip(feature_names, importances_gb):\n\tprint(f\"{name}: {imp:.3f}\")\nplt.figure(figsize=(6,3))\nplt.barh(feature_names, importances_gb)\nplt.title(\"Feature importances (GradientBoosting)\")\nplt.show()\n```","x":1264,"y":6360,"width":989,"height":920},
		{"id":"6ae21acbabbb4cee","type":"text","text":"```python\nn_estimators_list = [10, 50, 100, 200, 500]\nlearning_rates = [0.01, 0.05, 0.1, 0.2]\n\nresults_gb = []\nfor lr in learning_rates:\n\tfor n in n_estimators_list:\n\t\tgb = GradientBoostingClassifier(n_estimators=n, learning_rate=lr, max_depth=3, random_state=42)\n\t\tgb.fit(X_train, y_train)\n\t\tresults_gb.append((lr, n, accuracy_score(y_train, gb.predict(X_train)), accuracy_score(y_test, gb.predict(X_test))))\n\nimport pandas as pd\ndf_gb = pd.DataFrame(results_gb, columns=[\"learning_rate\",\"n_estimators\",\"train_acc\",\"test_acc\"])\npivot_gb = df_gb.pivot(index=\"n_estimators\", columns=\"learning_rate\", values=\"test_acc\")\nprint(\"GradientBoosting test accuracy (rows: n_estimators, cols: learning_rate):\")\n\n#display(pivot_gb)\nprint(pivot_gb)\n```","x":1264,"y":7440,"width":1140,"height":682},
		{"id":"f55a64099672e7a9","type":"file","file":"mashinoe/ml-labs/lab3/ТЗ - Лабораторная работа №3.md","x":-1760,"y":-645,"width":820,"height":549},
		{"id":"d002ed8b997bb164","type":"file","file":"mashinoe/ml-labs/lab3/пр3.pdf","x":-1760,"y":45,"width":820,"height":600},
		{"id":"9a81bd55282f72cc","x":680,"y":1080,"width":250,"height":60,"type":"text","text":"1. одно дерево"},
		{"id":"1765864164215b08","x":680,"y":5280,"width":250,"height":60,"type":"text","text":"2. случайный лес"},
		{"id":"ee8c2fbc5d9556aa","type":"text","text":"3. градиентный бустинг","x":680,"y":7280,"width":250,"height":60},
		{"id":"8cc217d93ebe11f1","x":-640,"y":2080,"width":710,"height":462,"type":"text","text":"2. Использование деревьев решений и их ансамблей для задачи классификации ирисов Фишера \n\nМы по-прежнему рассматриваем тот же набор данных (вам нужна тестовая и обучающая выборка). В этой части работы Вы используете методы, реализованные в sklearn. \nОбучите следующие классификаторы на основе деревьев решений: \n1) одно дерево, \nПокажите пример переобученного дерева и опишите, как Вы боретесь с этим явлением. \n2) случайный лес,\nКакие параметры есть у этого метода? Продемонстрируйте их влияние на эффективность модели. \n3) градиентный бустинг. \nКакие параметры есть у этого метода? Продемонстрируйте их влияние на эффективность модели."},
		{"id":"622596d0e0a8b98b","x":3920,"y":-679,"width":1020,"height":731,"type":"text","text":"# Что делает код\n# Библиотеки\n- `numpy` — базовая библиотека для чисел и массивов (здесь пока не используется явно, но принято импортировать для ML-задач).\n    \n- `matplotlib.pyplot` — для построения графиков (импортировано заранее для визуализаций).\n    \n- `load_iris` — функция из sklearn, которая загружает стандартный набор данных «Iris».\n    \n- `train_test_split` — утилита для случайного разбиения данных на обучающую и тестовую выборки.\n    \n- `accuracy_score`, `classification_report`, `confusion_matrix`, `ConfusionMatrixDisplay` — набор инструментов для оценки качества классификаторов (accuracy, подробный отчёт по precision/recall/f1, матрица ошибок и её удобный отрисовщик).\n\n# Абзац 1\n\n- `iris` — объект с данными; в нём есть `data` (матрица признаков), `target` (метки классов), `feature_names` (имена признаков) и `target_names` (имена классов).\n    \n- `X = iris.data` — присваиваем матрицу признаков переменной `X`. В Iris это shape `(150, 4)`.\n    \n- `y = iris.target` — вектор меток размера 150.\n    \n- `feature_names` и `target_names` — удобные строковые подписи для отчётов/графиков.\n\n# Абзац 2 (стратифицированное разбиение)\n\n`train_test_split` делит данные на обучающую и тестовую выборки.\n\n- `test_size=0.2` — 20% данных уходит в тест (т.е. 30 объектов для Iris).\n    \n- `random_state=42` — фиксирует «семя» генератора случайных чисел, чтобы разбиение было воспроизводимым.\n    \n- `stratify=y` — важный параметр: гарантирует, что распределение классов в train и test будет таким же, как в исходных данных (т.е. сохранится соотношение 0/1/2). Это особенно нужно для небольших датасетов и для корректной оценки."},
		{"id":"e10d20bd0392c464","x":3920,"y":126,"width":1020,"height":786,"type":"text","text":"# Что делает код\n\n`from sklearn.tree import DecisionTreeClassifier, plot_tree`\n\nИмпортируем класс для дерева решений и вспомогательную функцию для его визуализации.\n\n`dt_full = DecisionTreeClassifier(random_state=42)`\n\nСоздаём дерево без ограничений по глубине и другим параметрам; `random_state=42` — для воспроизводимости структуры при равных условиях.\n\n`dt_full.fit(X_train, y_train)`\n\nОбучаем дерево на тренировочной выборке. Для деревьев «обучение» — это построение структуры ветвлений, которая полностью подгоняет правила разделения под данные.\n\n`train_acc = accuracy_score(y_train, dt_full.predict(X_train)) test_acc = accuracy_score(y_test, dt_full.predict(X_test))`\n\nВычисляем точность (accuracy):\n\n- `train_acc` — насколько дерево правильно предсказывает те же данные, на которых обучалось;\n    \n- `test_acc` — насколько оно правильно работает на отложенной тестовой выборке.\n    \n\n`print(\"Неограниченное дерево - train acc: {:.4f}, test acc: {:.4f}\".format(train_acc, test_acc))`\n\nПечатаем значения точности с четырьмя знаками после запятой.\n\n`plt.figure(figsize=(12,6)) plot_tree(dt_full, feature_names=feature_names, class_names=target_names, filled=True, max_depth=3) plt.title(\"Неограниченное дерево решений\") plt.show()`\n\nВизуализируем построенное дерево. Параметр `max_depth=3` **только** ограничивает глубину отрисовки (для читаемости) — само дерево по-прежнему может быть глубже. `filled=True` раскрашивает узлы по доминирующему классу."},
		{"id":"52f2d7a0abf9bd56","x":3920,"y":1326,"width":1160,"height":600,"type":"text","text":"# Что делает код\n\n```\nfor params in [\n    {\"max_depth\": 2},\n    {\"max_depth\": 3},\n    {\"max_depth\": 4},\n    {\"min_samples_leaf\": 5},\n    {\"min_samples_split\": 5}\n]:\n```\n\nПеребирается список словарей с разными наборами параметров params. Для каждого набора вы будете создавать дерево с этими параметрами.\n\n    dt = DecisionTreeClassifier(random_state=42, **params)\n\n\nСоздаёте DecisionTreeClassifier, передавая в него текущее params. random_state=42 даёт воспроизводимость.\n\n    dt.fit(X_train, y_train)\n\n\nОбучаете дерево на тренировочных данных.\n\n    print(f\"Params: {params} -> train_acc={accuracy_score(y_train, dt.predict(X_train)):.4f}, test_acc={accuracy_score(y_test, dt.predict(X_test)):.4f}\")\n\n\nДля текущего дерева вычисляете и печатаете accuracy на тренировочной и на тестовой выборках."},
		{"id":"baa46598244794bb","x":5760,"y":-645,"width":543,"height":418,"type":"text","text":"### Вывод\n```\nTrain shape: (120, 4) Test shape: (30, 4)\n```\n- Всего было 150 образцов, 4 признака у каждого.\n    \n- При `test_size=0.2` тестовая часть — 20% от 150 = 30 образцов. Соответственно train = 150 − 30 = 120.\n    \n- Форма `(120, 4)` означает 120 строк (объектов) и 4 столбца (признаков) в обучающей матрице; `(30, 4)` — аналогично для тестовой.\n    \n- Этот вывод подтверждает, что `train_test_split` с заданными аргументами сработал как ожидалось.v"},
		{"id":"aad98be9bcd54070","x":5760,"y":126,"width":543,"height":485,"type":"text","text":"# Вывод\n\n`Неограниченное древо - train acc: 1.0000, test acc: 0.9333`\n\n- `train_acc = 1.0000` (100% на тренировке) — дерево полностью запомнило тренировочные примеры. Это обычный признак **переобучения** (overfitting) у неограниченных деревьев: они могут строить очень глубокие правила, чтобы верно классифицировать все обучающие объекты.\n    \n- `test_acc = 0.9333` (≈93.33% на тесте) — дерево всё ещё хорошо обобщает, но точность ниже, чем на train. Разрыв (1.00 → 0.9333) подтверждает, что модель частично переобучилась (подогналась под шум/детали тренировочной выборки).\n    \n\nВ целом: результат ожидаемый — глубокое дерево даёт идеальную точность на train и хорошую, но ниже — на test."},
		{"id":"ad523c78addc16d7","x":5760,"y":1326,"width":680,"height":740,"type":"text","text":"## Вывод\n\n```\nParams: {'max_depth': 2} -> train_acc=0.9667, test_acc=0.9333 \nParams: {'max_depth': 3} -> train_acc=0.9833, test_acc=0.9667 \nParams: {'max_depth': 4} -> train_acc=0.9917, test_acc=0.9333 \nParams: {'min_samples_leaf': 5} -> train_acc=0.9667, test_acc=0.9333 \nParams: {'min_samples_split': 5} -> train_acc=0.9833, test_acc=0.9667\n```\n\n- `max_depth=2`  \n    — относительно простое дерево. Train 96.67%, Test 93.33%. Хорошая обобщающая способность, небольшая недообученность/баланс.\n    \n- `max_depth=3`  \n    — чуть более сложное дерево. Train 98.33%, Test 96.67%. Здесь мы видим наилучший компромисс: высокая точность на train и ещё более высокая на test — именно `max_depth=3` в данном эксперименте дал **лучший результат на тесте**.\n    \n- `max_depth=4`  \n    — ещё более сложное дерево. Train 99.17% (очень близко к 100%), но Test 93.33% (упала по сравнению с depth=3). Это признак **переобучения**: дерево подстроилось под детали train и хуже обобщает.\n    \n- `min_samples_leaf=5`  \n    — ограничение минимального числа образцов в листе; это регуляризация (не даёт очень маленьким листьям). Результат: train 96.67%, test 93.33% — похоже на `max_depth=2` по поведению: менее сложная модель, чуть более устойчивая, но не достигла пика `max_depth=3`.\n    \n- `min_samples_split=5`  \n    — требует минимум 5 образцов для разбиения узла. Даёт train 98.33% и test 96.67% — по тесту такое поведение аналогично `max_depth=3` и также даёт отличный компромисс."},
		{"id":"b1034f67d0608c57","x":5760,"y":2360,"width":720,"height":340,"type":"text","text":"### Вывода\n\n`Unrestricted tree -> train acc: 1.0000, test acc: 0.9333`\n`Limited tree (max_depth=3) -> train acc: 0.9833, test acc: 0.9667`\n\n- **Unrestricted:** идеальная точность на обучении (1.0) и заметно ниже на тесте → явный признак переобучения (модель «запомнила» тренировочные примеры).\n    \n- **Limited (max_depth=3):** небольшое снижение train-accuracy (0.9833) и **улучшение test-accuracy (0.9667)** — значит регуляризация (ограничение глубины) успешно снизила переобучение и улучшила обобщение.\n    \n- Вывод: небольшая потеря на train оправдана значительным приростом качества на новых данных — это желаемый эффект регуляризации."},
		{"id":"49c2ac556ae11996","x":3920,"y":2360,"width":720,"height":280,"type":"text","text":"### Что делает код\n\n1. Обучает два дерева на одних и тех же `X_train,y_train`: одно **без ограничений** (`dt_unrestricted`), второе с жестким ограничением глубины `max_depth=3` (`dt_limited`).\n    \n2. Считает и печатает accuracy на train и test для обоих деревьев.\n    \n3. Рисует рядом (слева — первые 3 уровня неограниченного дерева, справа — полную структуру ограниченного дерева) для наглядного сравнения сложности модели."},
		{"id":"ce3284583518e855","x":3920,"y":4300,"width":720,"height":396,"type":"text","text":"## Что делает код\n\n1. `from sklearn.ensemble import RandomForestClassifier` — импорт классификатора ансамбля деревьев.\n    \n2. `rf = RandomForestClassifier(n_estimators=100, random_state=42)` — создаётся случайный лес из 100 деревьев (по умолчанию деревья не ограничены по глубине).\n    \n3. `rf.fit(X_train, y_train)` — «обучение» (в случае RF — построение множества деревьев на бутстреп-выборках).\n    \n4. `accuracy_score(...)` — вычисляется точность на train и test и печатается.\n    \n5. `importances = rf.feature_importances_` — извлекаются важности признаков, оценённые ансамблем (сумма = 1.0).\n    \n6. Далее печатаются имена признаков с их важностями и строится горизонтальная столбчатая диаграмма."},
		{"id":"a28138aca55156b1","x":5760,"y":4300,"width":928,"height":474,"type":"text","text":"## Вывод\n\n```\nRandomForest (n_estimators=100) -> train acc: 1.0000, test acc: 0.9000 \nsepal length (cm): 0.116 \nsepal width (cm): 0.015 \npetal length (cm): 0.431 \npetal width (cm): 0.437\n```\n\n- **Train acc = 1.0000** — модель идеально классифицирует обучающие данные. Для RandomForest это возможно на маленьком датасете (Iris) при default-параметрах: каждое дерево может переобучиться на своей бутстреп-выборке, а ансамбль в сумме «запоминает» почти всё.\n    \n- **Test acc = 0.9000** — на отложенной выборке качество ниже (90%), значит есть некоторый разрыв train→test; это указывает на **частичное переобучение** или просто на ограниченную выборку для оценки.\n    \n- **Feature importances**: `petal length` и `petal width` доминируют (~0.43 и ~0.44), `sepal length` имеет умеренное значение (~0.116), `sepal width` почти не важен (~0.015). Это согласуется с визуализацией: петал-параметры несут основную информацию для разделения классов."},
		{"id":"0ce2ff29c9ead9e0","x":3920,"y":5440,"width":720,"height":280,"type":"text","text":"## Что делает код\n\n1. Перебирает комбинации `n_estimators` и `max_depth`.\n    \n2. Для каждой комбинации обучает `RandomForestClassifier(n_estimators=n, max_depth=d)` на `X_train,y_train`.\n    \n3. Сохраняет `train_acc` и `test_acc` в список `results`.\n    \n4. Преобразует результаты в `DataFrame` и делает `pivot` по `n_estimators` (строки) и `max_depth` (столбцы), показывая **test_acc**."},
		{"id":"771fd23ba17ba85b","x":5760,"y":5440,"width":928,"height":440,"type":"text","text":"## Вывод (pivot-таблицы)\n\n- Столбец `NaN` соответствует `max_depth=None` (т. е. неограниченная глубина).\n    \n- Числа в ячейках — accuracy на тесте для соответствующей пары `(n_estimators, max_depth)`.\n    \n- Из таблицы видно, что **наиболее стабильные и высокие test_acc ≈ 0.966667** получаются при `max_depth=3` или `max_depth=4` для большинства `n_estimators` (особенно для n ≥ 10).\n    \n- `n_estimators` от 10 до 200 даёт примерно одинаковый test-результат → после некоторого количества деревьев добавлять ещё сильно не улучшает качество, но уменьшает разброс.\n    \n- `max_depth=None` (NaN) в целом даёт чуть худшие результаты (0.90–0.9667), что говорит о склонности к переобучению при неограниченной глубине.\n    \n- Малые `n_estimators` (1,5) дают более вариабельные результаты, но всё ещё могут достигать 0.9667 при удачных `max_depth`.\n    \n\n**Короткий вывод:** оптимальная зона — `max_depth` около 3–4 и `n_estimators` уже от ~10; увеличение `n_estimators` сверх ~50 редко даёт прибавку в тест-accuracy на этом датасете."},
		{"id":"7d651e7e43d9852c","x":3920,"y":6360,"width":884,"height":720,"type":"text","text":"## Что делает код\n\n`from sklearn.ensemble import GradientBoostingClassifier`\n\nИмпорт классификатора градиентного бустинга.\n\n`gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)`\n\nСоздаётся модель с 100 базовыми деревьями, шагом обучения `learning_rate=0.1`, глубиной деревьев `max_depth=3`. `random_state` — для воспроизводимости.\n\n- `n_estimators` — число итераций/деревьев;\n    \n- `learning_rate` — масштаб поправок каждого следующего дерева (меньше — медленнее, но надёжнее);\n    \n- `max_depth` — глубина каждого базового дерева (контроль сложности).\n    \n\n`gb.fit(X_train, y_train)`\n\nОбучение: каждое дерево обучается последовательно, исправляя ошибки ансамбля на предыдущих шагах.\n\n`print(\"GradientBoosting (100, lr=0.1, depth=3) -> train acc: {:.4f}, test acc: {:.4f}\".format(     accuracy_score(y_train, gb.predict(X_train)), accuracy_score(y_test, gb.predict(X_test)) ))`\n\nВычисляется и печатается точность на train и test — быстрый sanity-check качества и переобучения.\n\n`importances_gb = gb.feature_importances_ for name, imp in zip(feature_names, importances_gb):     print(f\"{name}: {imp:.3f}\") plt.figure(figsize=(6,3)) plt.barh(feature_names, importances_gb) plt.title(\"Feature importances (GradientBoosting)\") plt.show()`\n\n- `feature_importances_` — встроённая оценка вклада каждого признака (суммируется в 1).\n    \n- Печатаем значения и строим горизонтальную столбчатую диаграмму для наглядности."},
		{"id":"70e3f53643d9c51c","x":5760,"y":6360,"width":928,"height":721,"type":"text","text":"## Интерпретация вывода\n\n```\nGradientBoosting (100, lr=0.1, depth=3) -> train acc: 1.0000, test acc: 0.9667 \nsepal length (cm): 0.003 \nsepal width (cm): 0.016 \npetal length (cm): 0.331 \npetal width (cm): 0.650\n```\n\n- **Train acc = 1.0000** — модель идеально предсказывает обучающие данные.\n    \n- **Test acc = 0.9667** — высокая, но ниже, чем на train. Разрыв указывает на **частичное переобучение** (особенно при small dataset как Iris). В целом результат хороший: GB правильно обобщает большинство примеров.\n    \n- **Важности признаков**: `petal width` (0.65) и `petal length` (0.331) доминируют; `sepal`-параметры почти не влияют. Это согласуется с EDA: петал-признаки дают основную разделяющую информацию.\n    \n\n---\n\n## Объяснение графика (feature importances)\n\n- Горизонтальная столбчатая диаграмма показывает относительную важность каждого признака: более длинная полоса → признак сильнее уменьшает ошибку при разделении.\n    \n- На графике визуально очевидно, что два параметра лепестка (petal width, petal length) существенно важнее чашелистика.\n    \n- Для отчёта: вставьте график рядом с табличным выводом importances — это наглядно подтверждает выводы."},
		{"id":"3ffda6ce987516b7","x":3920,"y":7440,"width":884,"height":1080,"type":"text","text":"# Что делает код\n\n`n_estimators_list = [10, 50, 100, 200, 500] learning_rates = [0.01, 0.05, 0.1, 0.2]`\n\nСписки параметров, которые будем проверять.\n\n`results_gb = []`\n\nПустой список для хранения результатов.\n\n`for lr in learning_rates:     for n in n_estimators_list:`\n\nДва цикла — перебираем все комбинации _(4 learning_rate × 5 n_estimators)_.\n\n        `gb = GradientBoostingClassifier(n_estimators=n, learning_rate=lr, max_depth=3, random_state=42)`\n\nСоздаём модель градиентного бустинга с текущими параметрами.\n\n        `gb.fit(X_train, y_train)`\n\nОбучаем модель.\n\n        `results_gb.append((lr, n, accuracy_score(y_train, gb.predict(X_train)), accuracy_score(y_test, gb.predict(X_test))))`\n\nСохраняем:\n\n- lr\n    \n- n\n    \n- точность на train\n    \n- точность на test\n    \n\n`df_gb = pd.DataFrame(results_gb, columns=[\"learning_rate\",\"n_estimators\",\"train_acc\",\"test_acc\"])`\n\nПереводим всё в таблицу.\n\n`pivot_gb = df_gb.pivot(index=\"n_estimators\", columns=\"learning_rate\", values=\"test_acc\")`\n\nСоздаём сводную таблицу:  \nстроки — n_estimators  \nстолбцы — learning_rate  \nзначения — test accuracy."},
		{"id":"c1ccc15dd3f0294f","x":5760,"y":7440,"width":928,"height":160,"type":"text","text":"# Вывод\n\n**Код перебирает комбинации learning_rate и n_estimators, обучает GradientBoostingClassifier, измеряет точности и выводит таблицу, где видно, что на датасете Iris почти все модели показывают одинаковую test accuracy ≈ 0.9667.**"}
	],
	"edges":[
		{"id":"aa5fe815b970abbf","fromNode":"c0fcdf6f3b37c006","fromSide":"bottom","toNode":"7e3a95c99197c4bc","toSide":"top"},
		{"id":"10cecb7b89774c0f","fromNode":"7e3a95c99197c4bc","fromSide":"bottom","toNode":"98c19f21f60a47a4","toSide":"top"},
		{"id":"0938421655694ab9","fromNode":"98c19f21f60a47a4","fromSide":"bottom","toNode":"08420326263ddaa2","toSide":"top"},
		{"id":"a87cba32b408cf06","fromNode":"08420326263ddaa2","fromSide":"bottom","toNode":"a4289d9c8d58cc13","toSide":"top"},
		{"id":"0b9a4ad22d8c2677","fromNode":"a4289d9c8d58cc13","fromSide":"bottom","toNode":"ffe9cba4615b37ee","toSide":"top"},
		{"id":"b21e158a775da71d","fromNode":"ffe9cba4615b37ee","fromSide":"bottom","toNode":"23544791ea73694a","toSide":"top"},
		{"id":"3237512f1d302b34","fromNode":"23544791ea73694a","fromSide":"bottom","toNode":"6ae21acbabbb4cee","toSide":"top"},
		{"id":"017c587fef1bb3f3","fromNode":"d002ed8b997bb164","fromSide":"right","toNode":"8cc217d93ebe11f1","toSide":"left"},
		{"id":"5f49eed2780999c1","fromNode":"9a81bd55282f72cc","fromSide":"right","toNode":"7e3a95c99197c4bc","toSide":"left"},
		{"id":"2da3185e2310023a","fromNode":"9a81bd55282f72cc","fromSide":"right","toNode":"98c19f21f60a47a4","toSide":"left"},
		{"id":"0aebe7432602e476","fromNode":"9a81bd55282f72cc","fromSide":"right","toNode":"08420326263ddaa2","toSide":"left"},
		{"id":"09b739826cc33fad","fromNode":"9a81bd55282f72cc","fromSide":"right","toNode":"c0fcdf6f3b37c006","toSide":"left"},
		{"id":"4b46d4cb6e53053f","fromNode":"8cc217d93ebe11f1","fromSide":"right","toNode":"9a81bd55282f72cc","toSide":"left"},
		{"id":"96f76b627405099a","fromNode":"8cc217d93ebe11f1","fromSide":"right","toNode":"1765864164215b08","toSide":"left"},
		{"id":"ad88b7aa80e22fb1","fromNode":"1765864164215b08","fromSide":"right","toNode":"a4289d9c8d58cc13","toSide":"left"},
		{"id":"a6d7cb1796eae232","fromNode":"1765864164215b08","fromSide":"right","toNode":"ffe9cba4615b37ee","toSide":"left"},
		{"id":"64e1f875d4a7d8bd","fromNode":"8cc217d93ebe11f1","fromSide":"right","toNode":"ee8c2fbc5d9556aa","toSide":"left"},
		{"id":"e241c2029ca3910e","fromNode":"ee8c2fbc5d9556aa","fromSide":"right","toNode":"23544791ea73694a","toSide":"left"},
		{"id":"e07eb859c28a61a6","fromNode":"ee8c2fbc5d9556aa","fromSide":"right","toNode":"6ae21acbabbb4cee","toSide":"left"},
		{"id":"bdf409f047332017","fromNode":"c0fcdf6f3b37c006","fromSide":"right","toNode":"622596d0e0a8b98b","toSide":"left"},
		{"id":"f579fb6241e10261","fromNode":"622596d0e0a8b98b","fromSide":"right","toNode":"baa46598244794bb","toSide":"left"},
		{"id":"f155d039621c348d","fromNode":"7e3a95c99197c4bc","fromSide":"right","toNode":"e10d20bd0392c464","toSide":"left"},
		{"id":"04725356fb8834bc","fromNode":"e10d20bd0392c464","fromSide":"right","toNode":"aad98be9bcd54070","toSide":"left"},
		{"id":"eefba249bc25b201","fromNode":"98c19f21f60a47a4","fromSide":"right","toNode":"52f2d7a0abf9bd56","toSide":"left"},
		{"id":"d59e05d9b8f20c15","fromNode":"52f2d7a0abf9bd56","fromSide":"right","toNode":"ad523c78addc16d7","toSide":"left"},
		{"id":"6eaf3fa37610c674","fromNode":"08420326263ddaa2","fromSide":"right","toNode":"49c2ac556ae11996","toSide":"left"},
		{"id":"16be6e7484749886","fromNode":"49c2ac556ae11996","fromSide":"right","toNode":"b1034f67d0608c57","toSide":"left"},
		{"id":"9b2dad441aaefbfb","fromNode":"a4289d9c8d58cc13","fromSide":"right","toNode":"ce3284583518e855","toSide":"left"},
		{"id":"6e76754e3aecc316","fromNode":"ce3284583518e855","fromSide":"right","toNode":"a28138aca55156b1","toSide":"left"},
		{"id":"d5aa27ac77953038","fromNode":"ffe9cba4615b37ee","fromSide":"right","toNode":"0ce2ff29c9ead9e0","toSide":"left"},
		{"id":"7fd309e4022e0e9b","fromNode":"0ce2ff29c9ead9e0","fromSide":"right","toNode":"771fd23ba17ba85b","toSide":"left"},
		{"id":"78793a631c53b235","fromNode":"23544791ea73694a","fromSide":"right","toNode":"7d651e7e43d9852c","toSide":"left"},
		{"id":"ccce070c2f89adf1","fromNode":"6ae21acbabbb4cee","fromSide":"right","toNode":"3ffda6ce987516b7","toSide":"left"},
		{"id":"3cbbfb739bac7707","fromNode":"3ffda6ce987516b7","fromSide":"right","toNode":"c1ccc15dd3f0294f","toSide":"left"}
	]
}